# é æ¸¬FAD/FMN Binding-Site

## Introduction

é€™å€‹å°ˆé¡Œä¸»è¦æ˜¯é€éå„ç¨®æ©Ÿå™¨å­¸ç¿’ä¾†é æ¸¬FAD/FMN Binding Site
ç›®å‰å®Œæˆ: Transformer(Fine-tune model facebook/esm2_t6_8M_UR50D)

## Notice
Fine-tuneçš„æ¨¡å‹éå¤§ï¼Œç„¡æ³•æ¨é€è‡³githubï¼Œç¬¬ä¸€æ¬¡é‹è¡Œéœ€è¦åœ¨main.pyä¸­ä½¿ç”¨
```
transformers_trainer.run()
```
ä»¥å¾—åˆ°æ¨¡å‹

åƒè€ƒæ™‚é–“:

GPU: NVIDIA GeForce RTX 4060 Laptop GPU

æ™‚é–“: About 40 minutes

## ç›®éŒ„çµæ§‹(From ChatGPT):
```
protein_ml_project/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml             # åƒæ•¸è¨­å®šï¼ˆæ¨¡å‹ã€è¨“ç·´ã€è³‡æ–™ç­‰ï¼‰
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # åŸå§‹è›‹ç™½è³ªåºåˆ—æˆ–FASTAæª”æ¡ˆ
â”‚   â”œâ”€â”€ processed/              # è™•ç†å¾Œçš„è³‡æ–™é›†ï¼ˆCSVã€Numpyã€Pickleç­‰ï¼‰
â”‚   â””â”€â”€ features/               # ç‰¹å¾µè³‡æ–™ï¼ˆembeddingã€AACã€PSSMç­‰ï¼‰
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA.ipynb               # åˆæ­¥æ¢ç´¢æ€§åˆ†æ
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ load_data.py        # è›‹ç™½è³ªè³‡æ–™è¼‰å…¥èˆ‡é è™•ç†
â”‚   â”‚   â””â”€â”€ feature_engineering.py  # ç‰¹å¾µèƒå–ï¼Œå¦‚ one-hotã€ProtBERT ç­‰
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ rf_model.py         # Random Forest æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ cnn_model.py        # CNN æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ transformer.py      # Transformer æ¨¡å‹ï¼ˆé ç•™ Mamba æ¶æ§‹ï¼‰
â”‚   â”‚   â””â”€â”€ train.py            # è¨“ç·´é€šç”¨é‚è¼¯ï¼ˆæ”¯æ´ä¸åŒæ¨¡å‹ï¼‰
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ metrics.py          # è©•ä¼°æŒ‡æ¨™ã€æ··æ·†çŸ©é™£ç­‰
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helper.py           # é€šç”¨å·¥å…·ï¼ˆloggingã€seed è¨­å®šç­‰ï¼‰
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_2025_04_15_rf.yaml  # å¯¦é©—ç´€éŒ„
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/                 # å­˜æ”¾è¨“ç·´å¥½çš„æ¨¡å‹æª”
â”‚   â”œâ”€â”€ logs/                   # è¨“ç·´éç¨‹è¨˜éŒ„
â”‚   â””â”€â”€ results/                # çµæœè¼¸å‡ºï¼ˆåœ–è¡¨ã€CSVç­‰ï¼‰
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ main.py                    # å¯é¸çš„ä¸»å…¥å£è…³æœ¬ï¼ˆæ”¯æ´ argparse æŒ‡å®šæ¨¡å‹ï¼‰
```

ğŸ’¡ æ¶æ§‹è¨­è¨ˆæ€è·¯èˆ‡å„ªé»
ğŸ” æ¨¡å‹æ¨¡çµ„åŒ–
æ¯ç¨®æ¨¡å‹ï¼ˆRF, CNN, Transformer, Mambaï¼‰éƒ½åœ¨ models/ ä¸‹ä½œç‚ºç¨ç«‹æª”æ¡ˆ

è¨“ç·´é‚è¼¯çµ±ä¸€ç”± train.py æ§åˆ¶ï¼Œå¯æ’å…¥ä¸åŒæ¨¡å‹

ğŸ“Š è³‡æ–™åˆ†å±¤
raw è³‡æ–™ â†” è™•ç†å¾Œè³‡æ–™ â†” ç‰¹å¾µè³‡æ–™ æ¸…æ¥šåˆ†é–‹

è³‡æ–™è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹ç‚ºç¨ç«‹æ¨¡çµ„ï¼Œä¾¿æ–¼é‡è¤‡ä½¿ç”¨

âš™ï¸ å¯è¨­å®šåŒ–
æ‰€æœ‰è¨“ç·´è¶…åƒèˆ‡è·¯å¾‘å­˜åœ¨ config.yamlï¼Œæ–¹ä¾¿å¯¦é©—æ¯”è¼ƒ

ğŸ§ª å¯¦é©—ç´€éŒ„
æ¯æ¬¡è¨“ç·´åƒæ•¸ã€æ¨¡å‹çµæœç”¨ YAML æˆ– JSON å­˜æ”¾åœ¨ experiments/

ğŸ”„ æœªä¾†æ“´å……å®¹æ˜“
ä½ å¯ä»¥å¾ˆè¼•é¬†åœ°åŠ å…¥ Mamba æ¨¡å‹ï¼Œåªéœ€åœ¨ models/mamba.py è£¡å®šç¾©å®ƒ

## Results

### Transformer:

```
               precision    recall  f1-score   support

           0       1.00      1.00      1.00     88813
           1       0.93      0.88      0.90      2889

    accuracy                           0.99     91702
   macro avg       0.96      0.94      0.95     91702
weighted avg       0.99      0.99      0.99     91702
```

## Reference
[Quang-Thai Ho, Trinh-Trung-Duong Nguyen, Nguyen Quoc Khanh Le, Yu-Yen Ou,
FAD-BERT: Improved prediction of FAD binding sites using pre-training of deep bidirectional transformers,
Computers in Biology and Medicine,
Volume 131,
2021,
104258,
ISSN 0010-4825,
https://doi.org/10.1016/j.compbiomed.2021.104258.](https://www.sciencedirect.com/science/article/abs/pii/S0010482521000524?casa_token=kEkRv3ranIUAAAAA:S5dHeB0riirLnAALU0PxfWk71ubyxDTLdXzXXxN45KTIWii8kmXzVG8uSOmnfr0UB8NCs00Mhg#sec10)

[Peng, F.Z., Wang, C., Chen, T. et al. PTM-Mamba: a PTM-aware protein language model with bidirectional gated Mamba blocks. Nat Methods (2025).](https://doi.org/10.1038/s41592-025-02656-9)

[Albert Gu, Tri Dao, Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
