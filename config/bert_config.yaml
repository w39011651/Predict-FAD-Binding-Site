# config/training_config.yaml
training_arguments:
  dataloader_num_workers: 4
  output_dir: "./results"
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  num_train_epochs: 40
  eval_strategy: "steps"
  save_strategy: "steps"
  logging_dir: "./logs"
  logging_steps: 10
  fp16: false
  optim: "adamw_torch"
  adam_epsilon: !!float 1e-8
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  report_to: "none"
  disable_tqdm: false
  seed: 42
  eval_steps: 20
  metric_for_best_model: "recall_1"
  load_best_model_at_end: true
  greater_is_better: true
  weight_decay: 0.001
  learning_rate: !!float 1e-5
  lr_scheduler_type: "cosine"